{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already in the correct path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import configparser\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import lightgbm as lgb\n",
    "from plotnine import ggplot, aes, geom_boxplot, labs, scale_color_manual, geom_point\n",
    "import configparser\n",
    "\n",
    "project_root = \"/home/dan1dr/zrive-ds-4q24-churn\"\n",
    "\n",
    "# Define the project root path\n",
    "current_wd = os.getcwd()\n",
    "\n",
    "# Change the working directory if necessary\n",
    "if current_wd != project_root:\n",
    "    print(f\"Changing working directory from {current_wd} to {project_root}\")\n",
    "    os.chdir(project_root)\n",
    "else:\n",
    "    print(\"Already in the correct path\")\n",
    "\n",
    "# Add 'src' directory to sys.path\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "if src_path not in sys.path:\n",
    "    print(f\"Adding {src_path} to sys.path\")\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Import the modules\n",
    "from db_connectors.bigquery_service import BigqueryService\n",
    "from data_gathering import data_gathering\n",
    "from utils.logger import get_logger\n",
    "from data_cleaning import data_cleaning\n",
    "from column_config import users_cols, diff_cols, keep_cols, transform_cols, target_col\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"src/params.ini\")\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "def get_initial_params():\n",
    "    \"\"\"\n",
    "    Loads all relevant parameters into a global variable. These parameters are then accessible to other functions in the script.\n",
    "    Each function can consume the parameters it requires for its operation.\n",
    "    This approach ensures centralized management and consistency of parameters across different functions.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return a value but populates a global variable\n",
    "        with necessary parameters.\n",
    "    \"\"\"\n",
    "    global train_from, train_to, end_date, logistic_regression_params, lightgbm_params\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"src/params.ini\")\n",
    "\n",
    "    train_from = config.get(\"PARAMS\", \"train_from\")\n",
    "    train_to = config.get(\"PARAMS\", \"train_to\")\n",
    "    end_date = config.get(\"PARAMS\", \"end_date\")\n",
    "    # Load Logistic Regression parameters\n",
    "    logistic_regression_params = {\n",
    "        \"penalty\": config.get(\"LOGISTIC_REGRESSION\", \"penalty\", fallback=\"l2\"),\n",
    "        \"C\": config.getfloat(\"LOGISTIC_REGRESSION\", \"C\", fallback=1.0),\n",
    "        \"solver\": config.get(\"LOGISTIC_REGRESSION\", \"solver\", fallback=\"saga\"),\n",
    "        \"max_iter\": config.getint(\"LOGISTIC_REGRESSION\", \"max_iter\", fallback=10000),\n",
    "    }\n",
    "\n",
    "    # Load LightGBM parameters\n",
    "    lightgbm_params = {\n",
    "        \"boosting_type\": config.get(\"LIGHTGBM\", \"boosting_type\", fallback=\"gbdt\"),\n",
    "        \"num_leaves\": config.getint(\"LIGHTGBM\", \"num_leaves\", fallback=12),\n",
    "        \"max_depth\": config.getint(\"LIGHTGBM\", \"max_depth\", fallback=-1),\n",
    "        \"learning_rate\": config.getfloat(\"LIGHTGBM\", \"learning_rate\", fallback=0.005),\n",
    "        \"n_estimators\": config.getint(\"LIGHTGBM\", \"n_estimators\", fallback=100),\n",
    "        \"random_state\": config.getint(\"LIGHTGBM\", \"random_state\", fallback=500),\n",
    "        \"colsample_bytree\": config.getfloat(\n",
    "            \"LIGHTGBM\", \"colsample_bytree\", fallback=0.64\n",
    "        ),\n",
    "        \"subsample\": config.getfloat(\"LIGHTGBM\", \"subsample\", fallback=0.7),\n",
    "        \"reg_alpha\": config.getint(\"LIGHTGBM\", \"reg_alpha\", fallback=0),\n",
    "        \"reg_lambda\": config.getint(\"LIGHTGBM\", \"reg_lambda\", fallback=1),\n",
    "    }\n",
    "\n",
    "\n",
    "get_initial_params()\n",
    "save_curves_path = \"src/models\"\n",
    "save_features_path = \"src/features\"\n",
    "save_target_path = \"src/target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from column_config import users_cols, diff_cols, keep_cols, transform_cols, target_col\n",
    "\n",
    "\n",
    "def feature_computation(\n",
    "    clean_data: pd.DataFrame,\n",
    "    train_from: str,\n",
    "    train_to: str,\n",
    "    logger,\n",
    "    keep_gap_month_churns: bool = False,\n",
    "    save_features_path: str = \"\",\n",
    "    save_target_path: str = \"\",\n",
    ") -> (pd.DataFrame, pd.Series, pd.DataFrame, pd.Series):\n",
    "    \"\"\"\n",
    "    Split data into train and test features set, aggregate the data into historical behavior for those cols needed.\n",
    "    It also joins it with already calculated features, and extract the needed target from 2 months ahead.\n",
    "    Args:\n",
    "        clean_data: The cleaned dataset with customer, month, and payment information.\n",
    "        train_from: The starting date of the training period.\n",
    "        train_to: The ending date of the training period.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame with computed features for model training.\n",
    "        Series: Pandas Series representing the target variable for train set.\n",
    "        DataFrame: Pandas DataFrame with computed features for model testing.\n",
    "        Series: Pandas Series representing the target variable for test set.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting feature computation\")\n",
    "\n",
    "    # TO-DO: Catch exceptions\n",
    "    # TO-DO: Potential unit tests validating same length for features/targets\n",
    "    # TO-DO: Instead of defining the cols every time import them somewhere else (they're need in data_cleaning also)\n",
    "\n",
    "    # Convert the train_from and train_to to datetime\n",
    "    train_from_dt = pd.to_datetime(train_from)\n",
    "    train_to_dt = pd.to_datetime(train_to)\n",
    "\n",
    "    # Filter train and test data before feature computation\n",
    "    test_from_dt = train_from_dt + pd.DateOffset(months=1)\n",
    "    test_to_dt = train_to_dt + pd.DateOffset(months=1)\n",
    "    target_train_month = train_to_dt + pd.DateOffset(months=2)\n",
    "    target_test_month = test_to_dt + pd.DateOffset(months=2)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Train computation from {train_from_dt} to {train_to_dt}. Target for {target_train_month}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Test computation from {test_from_dt} to {test_to_dt}. Target for {target_test_month}\"\n",
    "    )\n",
    "\n",
    "    compute_ready_data = clean_data[\n",
    "        users_cols + transform_cols + keep_cols + diff_cols + target_col\n",
    "    ].copy()\n",
    "    train_df, test_df = split_train_test(\n",
    "        compute_ready_data, train_from_dt, train_to_dt, test_from_dt, test_to_dt\n",
    "    )\n",
    "\n",
    "    # We would train with previous users for train, but we need to remove them from test\n",
    "    # as they wouldn't porbably\n",
    "    previous_churned_users_train = train_df[\n",
    "        (train_df[\"date\"] <= train_to_dt) & (train_df[target_col[0]] > 0)\n",
    "    ][\"customer_id\"].unique()\n",
    "    previous_churned_users_test = test_df[\n",
    "        (test_df[\"date\"] <= test_to_dt) & (test_df[target_col[0]] > 0)\n",
    "    ][\"customer_id\"].unique()\n",
    "    train_df = train_df[~train_df[\"customer_id\"].isin(previous_churned_users_train)]\n",
    "    test_df = test_df[~test_df[\"customer_id\"].isin(previous_churned_users_test)]\n",
    "\n",
    "    logger.info(\n",
    "        f\"Removing {len(previous_churned_users_train)} previous churned users from train set\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Removing {len(previous_churned_users_test)} previous churned users from test set\"\n",
    "    )\n",
    "    logger.info(f\"Unique customers in train: {train_df['customer_id'].nunique()}\")\n",
    "    logger.info(f\"Unique customers in test: {test_df['customer_id'].nunique()}\")\n",
    "\n",
    "    logger.info(\"Starting features and target computation\")\n",
    "    logger.info(f\"Initial number of features passed: {train_df.shape[1]}\")\n",
    "    logger.info(\"Starting computation\")\n",
    "\n",
    "    train_df_features = compute_features(train_df, target_col, train_to_dt)\n",
    "    test_df_features = compute_features(test_df, target_col, test_to_dt)\n",
    "    train_df_target = compute_target(\n",
    "        compute_ready_data, target_col, target_train_month, keep_gap_month_churns\n",
    "    )\n",
    "    test_df_target = compute_target(\n",
    "        compute_ready_data, target_col, target_test_month, keep_gap_month_churns\n",
    "    )\n",
    "    logger.info(f\"Final number of features computed: {train_df_features.shape[1]}\")\n",
    "    logger.info(f\"Length train data: {len(train_df_features)}\")\n",
    "    logger.info(f\"Length test data: {len(test_df_features)}\")\n",
    "    logger.info(\"Computation done!\")\n",
    "\n",
    "    # As there are customer that leave between the month we use for training and the target month\n",
    "    # We have to join the features and the targets and drop those that don't have target. By doing this,\n",
    "    # we exclude customer that churned in gap month or those with no corresponding record in the target dataset.\n",
    "    features_and_target_train = train_df_features.merge(\n",
    "        train_df_target, on=\"customer_id\", how=\"left\"\n",
    "    )\n",
    "    features_and_target_test = test_df_features.merge(\n",
    "        test_df_target, on=\"customer_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    features_and_target_train = features_and_target_train[\n",
    "        features_and_target_train[target_col[0]].notna()\n",
    "    ]\n",
    "\n",
    "    features_and_target_test = features_and_target_test[\n",
    "        features_and_target_test[target_col[0]].notna()\n",
    "    ]\n",
    "\n",
    "    # Split train and test features + target (squeeze into 1D array)\n",
    "    features = features_and_target_train.drop(columns=target_col + users_cols)\n",
    "    features_test = features_and_target_test.drop(columns=target_col + users_cols)\n",
    "    target = features_and_target_train[target_col].squeeze()\n",
    "    target_test = features_and_target_test[target_col].squeeze()\n",
    "\n",
    "    logger.info(f\"Features: {features.columns.tolist()}\")\n",
    "    logger.info(f\"Target: {target.name}\")\n",
    "    logger.info(\"Completed feature computation!\")\n",
    "\n",
    "    try:\n",
    "        features.to_parquet(f\"{save_features_path}/features.parquet\", index=False)\n",
    "        features_test.to_parquet(\n",
    "            f\"{save_features_path}/features_test.parquet\", index=False\n",
    "        )\n",
    "        target.to_frame().to_parquet(f\"{save_target_path}/target.parquet\", index=False)\n",
    "        target_test.to_frame().to_parquet(\n",
    "            f\"{save_target_path}/target_test.parquet\", index=False\n",
    "        )\n",
    "        logger.info(f\"Features saved on {save_features_path}\")\n",
    "        logger.info(f\"Targets saved on {save_target_path}\")\n",
    "    except:\n",
    "        logger.info(f\"Features not saved on {save_features_path}\")\n",
    "        logger.info(f\"Targets not saved on {save_target_path}\")\n",
    "\n",
    "    return features, target, features_test, target_test\n",
    "\n",
    "\n",
    "def split_train_test(\n",
    "    df: pd.DataFrame,\n",
    "    train_from: pd.Series,\n",
    "    train_to: pd.Series,\n",
    "    test_from: pd.Series,\n",
    "    test_to: pd.Series,\n",
    ") -> (pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Split data into train and test.\n",
    "    Args:\n",
    "        df: The clean dataset with the columns we want to use as features.\n",
    "        train_from: The starting date of the training period.\n",
    "        train_to: The ending date of the training period.\n",
    "        test_from: The starting date of the testing period.\n",
    "        test_to: The ending date of the testing period.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame with training months only.\n",
    "        DataFrame: Pandas DataFrame with testing months only.\n",
    "    \"\"\"\n",
    "    # Create date col to mix month and year\n",
    "    df[\"date\"] = pd.to_datetime(\n",
    "        df[\"YEAR\"].astype(str) + \"-\" + df[\"MONTH\"].astype(str) + \"-01\"\n",
    "    )\n",
    "\n",
    "    # Filter compute_data for the specific date intervals.\n",
    "    df = df[(df[\"date\"] >= train_from) & (df[\"date\"] <= test_to)]\n",
    "\n",
    "    train_df = df[(df[\"date\"] >= train_from) & (df[\"date\"] <= train_to)]\n",
    "    test_df = df[(df[\"date\"] >= test_from) & (df[\"date\"] <= test_to)]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def compute_features(\n",
    "    df: pd.DataFrame, target_col: list[str], train_to_dt: pd.Timestamp\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the features and adds them to the df.\n",
    "    Args:\n",
    "        df: The clean dataset with the columns we want to use as features.\n",
    "        target_col: Name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame with new computed variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # TO-DO: The rolling function is propagated backwards for each single month.\n",
    "    # we just need it for the last one (but for that we need past data also). Didn't find\n",
    "    # any option to do it with pandas\n",
    "\n",
    "    df = df.drop(columns=target_col)\n",
    "\n",
    "    df = df.sort_values(by=[\"customer_id\", \"date\"])\n",
    "    df = df.set_index(\"date\")\n",
    "\n",
    "    # Dynamically compute features for each col in transform_cols\n",
    "    for col in transform_cols:\n",
    "        df[f\"{col}_prev_month\"] = df.groupby(\"customer_id\")[col].shift(1)\n",
    "        # df[f\"{col}_prev_month\"] = df[f\"{col}_prev_month\"].fillna(0)\n",
    "        df[f\"{col}_avg_3_months\"] = compute_x_months_avg(df, col, 3)\n",
    "        df[f\"{col}_avg_6_months\"] = compute_x_months_avg(df, col, 6)\n",
    "        df[f\"{col}_std_3_months\"] = compute_x_months_std(df, col, 3)\n",
    "        df[f\"{col}_std_6_months\"] = compute_x_months_std(df, col, 6)\n",
    "\n",
    "    # Filter only the computation backwards from the last month\n",
    "    train_to_str = train_to_dt.strftime(\"%Y-%m-%d\")\n",
    "    df = df.loc[train_to_str]\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def compute_x_months_avg(\n",
    "    df: pd.DataFrame, col_name: list[str], months: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the mean of the last months for a column.\n",
    "    Args:\n",
    "        df: The clean dataset with the columns we want to use as features.\n",
    "        col_name: Name of the column to compute\n",
    "        months: Number of months we want to compute the feature back\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame the new computed column.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.groupby(\"customer_id\")[col_name]\n",
    "        .rolling(window=months, min_periods=1)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_x_months_std(\n",
    "    df: pd.DataFrame, col_name: list[str], months: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the mean of the last months for a column.\n",
    "    Args:\n",
    "        df: The clean dataset with the columns we want to use as features.\n",
    "        col_name: Name of the column to compute\n",
    "        months: Number of months we want to compute the feature back\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame the new computed column.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.groupby(\"customer_id\")[col_name]\n",
    "        .rolling(window=months, min_periods=1)\n",
    "        .std()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_target(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    target_month: pd.Series,\n",
    "    keep_gap_month_churns: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the target column for a df.\n",
    "    Args:\n",
    "        df: The clean dataset with all the data.\n",
    "        target_col: Name of the target column.\n",
    "        target_month: The date where the target has to be computed.\n",
    "        keep_gap_month_churns: A boolean parameter that determines the treatment of churns occurring in the gap month.\n",
    "        If True, these churns are treated as actual churns (assigned a value of 1);\n",
    "        otherwise, they are excluded from the analysis. Defaults to False.\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame with the customer_id and the target computed.\n",
    "    \"\"\"\n",
    "\n",
    "    drop_churn_between_month = target_month - pd.DateOffset(months=1)\n",
    "\n",
    "    target_df = df[\n",
    "        (df[\"date\"] == target_month) | (df[\"date\"] == drop_churn_between_month)\n",
    "    ][[\"customer_id\"] + target_col + [\"date\"]]\n",
    "\n",
    "    for col in target_col:\n",
    "        target_df[col].fillna(0, inplace=True)\n",
    "        target_df[col] = np.where(\n",
    "            ((target_df[col] > 0) & (target_df[\"date\"] == target_month)),\n",
    "            1,\n",
    "            target_df[col],\n",
    "        )\n",
    "        target_df[col] = np.where(\n",
    "            ((target_df[col] > 0) & (target_df[\"date\"] == drop_churn_between_month)),\n",
    "            2,\n",
    "            target_df[col],\n",
    "        )\n",
    "\n",
    "    if keep_gap_month_churns:\n",
    "        # Convert all values that are 2 (gap month churns) into 1 (real churns)\n",
    "        target_df[col] = np.where(target_df[col] == 2, 1, target_df[col])\n",
    "\n",
    "    # Exclude the records that are still marked as 2 (gap month churns) if keep_gap_month_churns is False\n",
    "    target_df = target_df[\n",
    "        (target_df[\"NUM_DAYS_LINE_TYPE_FIXE_POST_DEA\"] != 2)\n",
    "        & (target_df[\"date\"] != drop_churn_between_month)\n",
    "    ][[\"customer_id\"] + target_col]\n",
    "\n",
    "    target_df[target_col] = target_df[target_col].astype(\"int\")\n",
    "\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Started querying data\n",
      "INFO - Data succesfully retrieved! Length: 253077\n",
      "INFO - Starting cleaning data\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Dropping column Import_Rest_quota_disp\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Dropping column NUM_FIX_PORT\n",
      "INFO - Dropping column NUM_FIX_PORT_LAST_1_MONTH\n",
      "INFO - Dropping column NUM_FIX_PORT_LAST_3_MONTHS\n",
      "INFO - Dropping column NUM_FIX_PORT_LAST_6_MONTHS\n",
      "INFO - Should fillna\n",
      "INFO - Dropping column NUM_MOB_PORT\n",
      "INFO - Dropping column NUM_MOB_PORT_LAST_1_MONTH\n",
      "INFO - Dropping column NUM_MOB_PORT_LAST_3_MONTHS\n",
      "INFO - Dropping column NUM_MOB_PORT_LAST_6_MONTHS\n",
      "INFO - Dropping column NUM_MOB_PORT_REQS_LAST_1_MONTH\n",
      "INFO - Dropping column NUM_MOB_PORT_REQS_LAST_3_MONTHS\n",
      "INFO - Dropping column NUM_MOB_PORT_REQS_LAST_6_MONTHS\n",
      "INFO - Dropping column NUM_MOB_PORT_TRANS_CURR\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Dropping column NUM_PORT_OPER_DONO_MASMOVIL_GRP_LAST_THREE_MONTHS\n",
      "INFO - Dropping column NUM_PORT_REQS_OPER_DONO_MASMOVIL_GRP_LAST_THREE_MONTHS\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Dropping column cust_max_days_between_fix_port\n",
      "INFO - Dropping column cust_max_days_between_mob_port\n",
      "INFO - Dropping column cust_max_months_between_fix_port\n",
      "INFO - Dropping column cust_max_months_between_mob_port\n",
      "INFO - Dropping column cust_min_days_between_fix_port\n",
      "INFO - Dropping column cust_min_days_between_mob_port\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Should fillna\n",
      "INFO - Completed cleaning data!\n",
      "INFO -   customer_id MONTH  YEAR  dif_pago_final_prev_month  \\\n",
      "0     7674307    08  2023                      47.87   \n",
      "1     7619662    07  2023                      38.85   \n",
      "2     7142608    02  2023                       0.60   \n",
      "3     7616568    04  2023                      51.67   \n",
      "4     7674918    06  2023                       2.16   \n",
      "\n",
      "   dif_pago_final_prev_2_month  dif_pago_final_prev_3_month  \\\n",
      "0                       113.95                       113.95   \n",
      "1                        36.44                        46.67   \n",
      "2                         1.80                        -1.37   \n",
      "3                        54.30                        54.30   \n",
      "4                         2.16                         2.16   \n",
      "\n",
      "   dif_consumo_prev_month  dif_consumo_prev_2_month  dif_consumo_prev_3_month  \\\n",
      "0                  107.15                    317.09                    317.09   \n",
      "1                  -70.94                     -5.51                     -8.01   \n",
      "2                   -5.09                   -105.06                      0.74   \n",
      "3                 1438.11                   1506.60                   1506.60   \n",
      "4                    0.37                      0.37                      0.37   \n",
      "\n",
      "   dif_discount_prev_month  ...  PERC_CALL_TYPE_OUT_CURR  PERC_CALL_OWNN_CURR  \\\n",
      "0                    33.10  ...                50.072359             9.941520   \n",
      "1                   -49.98  ...                49.784017             2.915767   \n",
      "2                     5.69  ...                47.774159            41.042345   \n",
      "3                 -1424.43  ...                53.370787            41.171750   \n",
      "4                   -32.38  ...                69.565217            21.739130   \n",
      "\n",
      "   PERC_CALL_NATR_CURR  NUM_CALL_WEEK_CURR  NUM_CALL_WEEKEND_CURR  \\\n",
      "0            90.058480                 546                    145   \n",
      "1            97.084233                 719                    207   \n",
      "2            58.957655                 733                    188   \n",
      "3            58.828250                 966                    280   \n",
      "4            78.260870                  69                      0   \n",
      "\n",
      "   NUM_SECS_WEEK_CURR  NUM_SECS_WEEKEND_CURR  NUM_CALL_WEEK  NUM_CALL_WEEKEND  \\\n",
      "0               82710                  18697            459               121   \n",
      "1               75030                  15770            570               156   \n",
      "2              109977                  25655            600               162   \n",
      "3              329732                  27382            652               239   \n",
      "4               19222                      0             48                 0   \n",
      "\n",
      "   NUM_DAYS_LINE_TYPE_FIXE_POST_DEA  \n",
      "0                              <NA>  \n",
      "1                              <NA>  \n",
      "2                              <NA>  \n",
      "3                                 3  \n",
      "4                              <NA>  \n",
      "\n",
      "[5 rows x 175 columns]\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "    WITH all_periods AS (\n",
    "    SELECT * \n",
    "    FROM `mm-bi-catedras-upm.ESTIMACION_CHURN.multibrand_monthly_customer_base_mp2022`\n",
    "    UNION ALL \n",
    "    SELECT * \n",
    "    FROM `mm-bi-catedras-upm.ESTIMACION_CHURN.multibrand_monthly_customer_base_mp2023_1`\n",
    "    ), \n",
    "\n",
    "    selectable_customer AS (\n",
    "        SELECT customer_id\n",
    "        FROM all_periods\n",
    "        GROUP BY customer_id\n",
    "    ), \n",
    "\n",
    "    customer_selected AS (\n",
    "        SELECT customer_id AS selected_customer\n",
    "        FROM selectable_customer\n",
    "    WHERE MOD(ABS(FARM_FINGERPRINT(CAST(customer_id AS STRING))), 20) = 0\n",
    "    )\n",
    "\n",
    "    SELECT {\", \".join(diff_cols + keep_cols + users_cols + target_col + transform_cols)}\n",
    "    FROM all_periods\n",
    "    INNER JOIN customer_selected\n",
    "    ON customer_id = selected_customer\n",
    "    WHERE IS_CUST_SEGM_RESI > 0\n",
    "    AND IS_CUST_BILL_POST_CURR = TRUE\n",
    "    AND CUST_BUNDLE_CURR = 'FMC'\n",
    "    AND NUM_IMPAGOS = 0\n",
    "    AND pago_final_0 IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "get_initial_params()\n",
    "raw_data = data_gathering(query, logger)\n",
    "clean_data = data_cleaning(raw_data, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target, features_test, target_test = feature_computation(\n",
    "    clean_data,\n",
    "    train_from,\n",
    "    train_to,\n",
    "    logger,\n",
    "    save_features_path=save_features_path,\n",
    "    save_target_path=save_target_path,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zrive-ds-YCVm4i4L-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
